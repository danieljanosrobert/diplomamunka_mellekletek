{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gyakran változtatott konstansok\n",
    "\n",
    "# Stop word-ök használata\n",
    "USE_STOPWORDS = True\n",
    "\n",
    "# Elérési úton lévő mentett modell betöltése kiértékelésre\n",
    "LOAD_CHECKPOINT = False\n",
    "\n",
    "# Az optimális validációs veszteséggel rendelkező epoch-ban lévő modellel való visszatérés tanításból\n",
    "RESTORE_BEST_WEIGHTS = True\n",
    "\n",
    "# GPU használata. Érték változtatása esetén a Kernel újraindítandó\n",
    "USE_GPU = True\n",
    "\n",
    "# Checkpoint könyvtár alatti könyvtár\n",
    "CHECKPOINT_SUBDIR = ''\n",
    "\n",
    "# Könyvtár, mely a CHECKPOINT_SUBDIR alatt található. Ide történik a kiértékelés mentése, illetve innen töltődik be\n",
    "# a kiértékelendő modell. Ha a könyvtár nem létezik, akkor létrehozásra kerül.\n",
    "CHECKPOINT_PREFIX = 'c_sw'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importok\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from transformers import TFBertForSequenceClassification, AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Konstansok\n",
    "\n",
    "# Random seed-ek értéke\n",
    "SHUFFLE_RANDOM_STATE = 42\n",
    "TRAIN_RANDOM_STATE = 42\n",
    "TEST_RANDOM_STATE = 42\n",
    "\n",
    "# A program számára fontos fejlécek értéke az adatbázisban\n",
    "TEXT = 'Sentence'\n",
    "Y_HEADER = 'LABEL'\n",
    "\n",
    "# Az adatbázisban található értékek megfeleltetése a programban\n",
    "LABELS = {\n",
    "    \"SPORT\": 0,\n",
    "    \"VIDEÓJÁTÉK\": 1\n",
    "}\n",
    "\n",
    "# Maximális tokensorozat méret\n",
    "MAX_SEQUENCE_LENGTH = 64\n",
    "\n",
    "# Tanítás során alkalmazott batch méret\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Tanítás során alkalmazott epoch szám. Korai megállás esetén nem feltétlenül jut el a megadott értékig a program\n",
    "EPOCHS = 10\n",
    "\n",
    "# Maximum hány TRAIN dokumentum kerüljön tokenizálásra. None esetén mind\n",
    "TRAIN_PROCESSED_MAX_DOCUMENTS = None\n",
    "\n",
    "# Maximum hány TEST dokumentum kerüljön tokenizálásra. None esetén mind\n",
    "TEST_PROCESSED_MAX_DOCUMENTS = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GPU beállítása\n",
    "\n",
    "if not USE_GPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "if physical_devices:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Mappastruktúra beállítása\n",
    "\n",
    "path = 'checkpoints/' + CHECKPOINT_SUBDIR + CHECKPOINT_PREFIX\n",
    "\n",
    "ITER = \"1\"\n",
    "if not os.path.exists(path):\n",
    "  os.makedirs(path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Szöveg feldolgozására alkalmas metódusok\n",
    "\n",
    "URL_RE = 'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}([-a-zA-Z0-9()@:%_+.~#?&/=]*)'\n",
    "WHITELIST_RE = '[^a-zA-Z0-9íÍöÖüÜóÓőŐúÚáÁéÉűŰ]'\n",
    "\n",
    "def cleanse(i):\n",
    "    \"\"\"\n",
    "    Adatbázisban szereplő dokumentumok megtisztítása az URL-ektől és a nem-alfanumerikus karakterektől.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    i: int\n",
    "        A tisztítandó dokumentumnak az indexe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A megtisztított dokumentum.\n",
    "    \"\"\"\n",
    "\n",
    "    text = dataset[TEXT].iloc[i]\n",
    "    text = re.sub(URL_RE, ' ', text)\n",
    "    text = re.sub(WHITELIST_RE, ' ', text)\n",
    "    text = ' '.join([text])\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "def delete_empty_rows(dataset):\n",
    "    \"\"\"\n",
    "    Az adatbázist megtisztítja az üres soroktól.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: pandas.DataFrame\n",
    "        Az adatbázisból készített DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A megtisztított adatbázis.\n",
    "    \"\"\"\n",
    "\n",
    "    ids_to_delete = dataset.index[dataset[TEXT] == ' '].tolist()\n",
    "    return dataset.drop(ids_to_delete)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adatok előkészítése\n",
    "\n",
    "dataset = pd.read_csv('db/sport_or_e_sport.csv', sep=';', error_bad_lines=False)\n",
    "dataset[Y_HEADER] = dataset[Y_HEADER].map(LABELS)\n",
    "dataset = shuffle(dataset, random_state=SHUFFLE_RANDOM_STATE)\n",
    "dataset.info()\n",
    "\n",
    "dataset = delete_empty_rows(dataset)\n",
    "for i  in range(len(dataset.index)):\n",
    "    dataset[TEXT].iloc[i] = cleanse(i)\n",
    "\n",
    "dataset.head(5)\n",
    "X = dataset[TEXT].values\n",
    "y = dataset[Y_HEADER].values\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.8, random_state=TRAIN_RANDOM_STATE)\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_rem, y_rem, train_size=0.5, random_state=TEST_RANDOM_STATE)\n",
    "\n",
    "y_train_labels = y_train\n",
    "y_dev_labels = y_dev\n",
    "y_test_labels = y_test\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_dev = to_categorical(y_dev, 2)\n",
    "y_test = to_categorical(y_test, 2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adatok vizualizálása\n",
    "\n",
    "def plot_label_counts(y, title='y labels'):\n",
    "    \"\"\"\n",
    "    Adatok eloszlásának ábrázolása.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        Az adathalmaz címkéi, amelyre a vizualizálás történik.\n",
    "    title: str\n",
    "        Az elkészített ábra címe.\n",
    "    \"\"\"\n",
    "\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    b = dict(zip(unique, counts))\n",
    "    plt.barh(range(len(b)),  list(b.values()), align='center', color=['lightblue', 'lightgreen'])\n",
    "    y_values = [\"Sport\", \"Videójáték\"]\n",
    "    y_axis = np.arange(0, 2, 1)\n",
    "    plt.yticks(y_axis, y_values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Number of Samples in training Set')\n",
    "    plt.ylabel('Label')\n",
    "    ax = plt.gca()\n",
    "    for i, v in enumerate(b.values()):\n",
    "        plt.text(ax.get_xlim()[1]/100, i, str(v), color='blue', fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_label_counts(y_train_labels, 'Train eloszlas')\n",
    "plot_label_counts(y_dev_labels, 'Dev eloszlas')\n",
    "plot_label_counts(y_test_labels, 'Test eloszlas')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# huBERT betöltése a Hugging Faces Model Hub-ból\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\", num_labels=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stop word-ök letöltése\n",
    "\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stop word-ök alkalmazásának előkészítése\n",
    "\n",
    "all_stopwords = []\n",
    "\n",
    "def apply_stopwords(sentences):\n",
    "    \"\"\"\n",
    "    Stop word-ök alkalmazása magyar nyelvű korpuszra.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences: list of numpy.ndarray\n",
    "        Az adathalmazban található dokumentumokat tartalmazó lista, amelyen a stop word-ök alkalmazva lesznek.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        A stop word-öket már nem tartalmazó dokumentumokat tartalmazó lista.\n",
    "    \"\"\"\n",
    "\n",
    "    global all_stopwords\n",
    "    corpus = []\n",
    "    for sen in sentences:\n",
    "        sentence = sen.split()\n",
    "        all_stopwords = stopwords.words('hungarian')\n",
    "        whitelist = [\"ne\", \"nem\", \"se\", \"sem\"]\n",
    "        sentence = [word for word in sentence if (word.lower() not in all_stopwords or word.lower() in whitelist)\n",
    "                 and len(word) > 1]\n",
    "        sentence = ' '.join(sentence)\n",
    "        corpus.append(sentence)\n",
    "\n",
    "    return corpus\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dokumentumok tokenizálása\n",
    "\n",
    "def batch_encode(X):\n",
    "    \"\"\"\n",
    "    Tokenizálás a huBERT tokenizálóval.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: list of str\n",
    "        Az adathalmazban található dokumentumok listája, amelyek tokenizálásra kerülnek.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    transformers.tokenization_utils_base.BatchEncoding\n",
    "        A tokenizált dokumentumok.\n",
    "    \"\"\"\n",
    "\n",
    "    return bert_tokenizer.batch_encode_plus(\n",
    "    X,\n",
    "    truncation=True,\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    add_special_tokens=True, # add [CLS] and [SEP] tokens\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False, # not needed for this type of ML task\n",
    "    padding='max_length', # add 0 pad tokens to the sequences less than max_length\n",
    "    return_tensors='tf'\n",
    ")\n",
    "X_train = X_train.tolist() if not USE_STOPWORDS else apply_stopwords(X_train.tolist())\n",
    "X_dev = X_dev.tolist() if not USE_STOPWORDS else apply_stopwords(X_dev.tolist())\n",
    "X_test = X_test.tolist() if not USE_STOPWORDS else apply_stopwords(X_test.tolist())\n",
    "\n",
    "\n",
    "X_train = batch_encode(X_train[:TRAIN_PROCESSED_MAX_DOCUMENTS])\n",
    "X_dev = batch_encode(X_dev[:TEST_PROCESSED_MAX_DOCUMENTS])\n",
    "X_test = batch_encode(X_test[:TEST_PROCESSED_MAX_DOCUMENTS])\n",
    "y_train = y_train[:TRAIN_PROCESSED_MAX_DOCUMENTS]\n",
    "y_dev = y_dev[:TEST_PROCESSED_MAX_DOCUMENTS]\n",
    "y_test = y_test[:TEST_PROCESSED_MAX_DOCUMENTS]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Modell létrehozása\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Modell létrehozása, melynek inputja az input id-kat, illetve az attention mask-ot tartalmazó\n",
    "    réteg, tartalmazza a BERT modellt, kimeneti rétege osztályozára alkalmas.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    keras.engine.functional.Functional\n",
    "        Az elkészített modell.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(64,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input((64,), dtype=tf.int32, name='attention_mask')\n",
    "    output = bert_model([input_ids, attention_mask])[0]\n",
    "    output = tf.keras.layers.Dropout(rate=0.15)(output)\n",
    "    output = tf.keras.layers.Dense(2, activation='softmax')(output)\n",
    "    result = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    return result\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Modell adatainak kiiratása\n",
    "\n",
    "print(bert_model.config)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checkpoint callback beállítása a modell checkpontjainak lementéséhez\n",
    "\n",
    "checkpoint_path = path + 'cp.ckpt'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tanítás során alkalmazott metódusok\n",
    "\n",
    "def get_history_as_text(history, epoch):\n",
    "    \"\"\"\n",
    "    Visszaadja a kiírandó és fájlba mentendő stringet, amely a tanítás history-ját tartalmazza.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A kiírandó és fájlba mentendő stringet, amely a tanítás history-ját tartalmazza.\n",
    "    \"\"\"\n",
    "\n",
    "    return f'Epoch {epoch+1: <3}: loss: {format(history[\"loss\"][epoch], \".4f\")} - accuracy: {format(history[\"accuracy\"][epoch], \".4f\")} - val_loss: {format(history[\"val_loss\"][epoch], \".4f\")} - val_accuracy: {format(history[\"val_accuracy\"][epoch], \".4f\")}'\n",
    "\n",
    "def fit_model():\n",
    "    \"\"\"\n",
    "    Modell illesztését elvégző metódus. Amennyiben a LOAD_CHECKPOINT True, úgy illesztés helyett a mentett tanítást\n",
    "    tölti be és annak eredményével tér vissza.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    history: dict of fit results\n",
    "        Az illesztés során az epoch-ok információit tartalmazó dic, mely tartalmazza a\n",
    "        loss, az accuracy, a val_loss és a val_accuracy adatokat.\n",
    "    result: list of float\n",
    "        A modell kiértékelését tartalmazza a teszt adathalmazra.\n",
    "    predict: numpy.ndarray\n",
    "        A modell predikcióit tartalmazza a teszt adathalmazra.\n",
    "    \"\"\"\n",
    "\n",
    "    if not LOAD_CHECKPOINT:\n",
    "        early_stopping_callback = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1,\n",
    "            patience=2,\n",
    "            restore_best_weights=RESTORE_BEST_WEIGHTS)\n",
    "        if ITER == \"6\":\n",
    "            history = model.fit(\n",
    "                x=X_train.values(),\n",
    "                y=y_train,\n",
    "                validation_data=(X_dev.values(), y_dev),\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                callbacks=[early_stopping_callback, cp_callback]\n",
    "            )\n",
    "        else:\n",
    "            history = model.fit(\n",
    "                x=X_train.values(),\n",
    "                y=y_train,\n",
    "                validation_data=(X_dev.values(), y_dev),\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                callbacks=[early_stopping_callback]\n",
    "            )\n",
    "        with open(path + '.history', 'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)\n",
    "        text_file = open(path + \"history\" + ITER + \".txt\", \"w\")\n",
    "        print(len(history.history['loss']))\n",
    "        for i in range(len(history.history['loss'])):\n",
    "            history_text = get_history_as_text(history.history, i)\n",
    "            print(history_text)\n",
    "            text_file.writelines(history_text + '\\n')\n",
    "        text_file.close()\n",
    "    else:\n",
    "        model.load_weights(checkpoint_path)\n",
    "        history = pickle.load(open(path + '.history', \"rb\"))\n",
    "        for i in range(len(history['loss'])):\n",
    "            history_text = get_history_as_text(history, i)\n",
    "            print(history_text)\n",
    "    result = model.evaluate(X_test.values(), y_test)\n",
    "    predict = model.predict(X_test.values())\n",
    "    np_predict = np.argmax(predict,axis=1)\n",
    "    return history, result, np_predict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Kiértékelés vizualizálása szövegesen és ábrán\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "def evaluate(predict, result, y):\n",
    "    \"\"\"\n",
    "    A modell kiértékelése. Kiíratja az osztályozási eredményeket, a pontosságot, illetve az igazságmátrixot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predict: numpy.ndarray\n",
    "        A modell predikcióit tartalmazza a teszt adathalmazra.\n",
    "    result: list of float\n",
    "        A modell kiértékelését tartalmazza a teszt adathalmazra.\n",
    "    y: numpy.ndarray\n",
    "        A teszt adathalmaz címkéit tartalmazza.\n",
    "    \"\"\"\n",
    "\n",
    "    y_le = le.fit_transform(y[:TEST_PROCESSED_MAX_DOCUMENTS])\n",
    "    print('Classification report:')\n",
    "    print(classification_report(y_le, predict))\n",
    "    print(f'Accuracy: {accuracy_score(y_le, predict): >43}')\n",
    "    print(f'Accuracy from evaluation: {result[1]: >27}')\n",
    "    print('Confusion matrix:')\n",
    "    df_cm = pd.DataFrame(confusion_matrix(y_le, predict),\n",
    "                         index=[i for i in ['sport', 'videójáték']],\n",
    "                         columns=[i for i in ['sport', 'videójáték']])\n",
    "    if not LOAD_CHECKPOINT:\n",
    "        with open(path + \"results\" + ITER + \".txt\", \"w\") as text_file:\n",
    "            df_cm_string = df_cm.to_string(header=False, index=False)\n",
    "            text_file.write('Classification report:\\n')\n",
    "            text_file.write(classification_report(y_le, predict))\n",
    "            text_file.write(f'\\nAccuracy: {accuracy_score(y_le, predict): >43}\\n')\n",
    "            text_file.write(f'Accuracy from evaluation: {result[1]: >27}\\n')\n",
    "            text_file.write('\\nConfusion matrix:\\n')\n",
    "            text_file.write(df_cm_string)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.title(CHECKPOINT_PREFIX[:-1])\n",
    "    hm = sn.heatmap(df_cm, annot=True, fmt='g', cmap=\"Blues\")\n",
    "    hm.set(ylabel='True label', xlabel='Predicted label')\n",
    "    if not LOAD_CHECKPOINT:\n",
    "        plt.savefig(path + 'accuracy-' + format(result[1], \".4f\") + ITER + '.jpg')\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Modell illesztése és kiértékelése 6 iterációban.\n",
    "\n",
    "for i in range(1, 7):\n",
    "    ITER = str(i)\n",
    "    print(\"Iteration \", ITER)\n",
    "    history, result, predict = fit_model()\n",
    "    evaluate(predict, result, y_test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}