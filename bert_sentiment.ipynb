{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Gyakran változtatott konstansok\n",
    "\n",
    "# Mekkora kontextusra történjen az illesztés\n",
    "CONTEXT_LENGTH = None\n",
    "\n",
    "# A teljes adatbázis pozitív címkéjű adatainak hány százalékával történjen a futás\n",
    "POZITIV_PERCENTAGE_TO_KEEP = 100\n",
    "\n",
    "# A teljes adatbázis negatív címkéjű adatainak hány százalékával történjen a futás\n",
    "NEGATIV_PERCENTAGE_TO_KEEP = 50\n",
    "\n",
    "# A teljes adatbázis semleges címkéjű adatainak hány százalékával történjen a futás\n",
    "SEMLEGES_PERCENTAGE_TO_KEEP = 12\n",
    "\n",
    "# Stop word-ök használata\n",
    "USE_STOPWORDS = False\n",
    "\n",
    "# Elérési úton lévő mentett modell betöltése kiértékelésre\n",
    "LOAD_CHECKPOINT = False\n",
    "\n",
    "# A címkékhez tartozó adatok illesztés során kerüljenek kiegyensúlyozásra\n",
    "USE_CLASS_WEIGHTS = False\n",
    "\n",
    "# Az optimális validációs veszteséggel rendelkező epoch-ban lévő modellel való visszatérés tanításból\n",
    "RESTORE_BEST_WEIGHTS = False\n",
    "\n",
    "# GPU használata. Érték változtatása esetén a Kernel újraindítandó\n",
    "USE_GPU = True\n",
    "\n",
    "# Checkpoint könyvtár alatti könyvtár\n",
    "CHECKPOINT_SUBDIR = ''\n",
    "\n",
    "# Könyvtár, mely a CHECKPOINT_SUBDIR alatt található. Ide történik a kiértékelés mentése, illetve innen töltődik be\n",
    "# a kiértékelendő modell. Ha a könyvtár nem létezik, akkor létrehozásra kerül.\n",
    "CHECKPOINT_PREFIX = 'ri_12_0/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importok\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from transformers import TFBertForSequenceClassification, AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Konstansok\n",
    "\n",
    "# Random seed értéke\n",
    "SEED = 42\n",
    "\n",
    "# Könyvtárspecifikus random seed-ek értéke\n",
    "SHUFFLE_RANDOM_STATE = SEED\n",
    "TRAIN_RANDOM_STATE = SEED\n",
    "TEST_RANDOM_STATE = SEED\n",
    "RANDOM_SEED = SEED\n",
    "NP_SEED = SEED\n",
    "TF_SEED = SEED\n",
    "PYTHON_HASH_SEED = SEED\n",
    "\n",
    "# A program számára fontos fejlécek értéke az adatbázisban\n",
    "TEXT = 'Sentence'\n",
    "START_TOKEN = 'START'\n",
    "TOKEN_LEN = 'LEN'\n",
    "Y_HEADER = 'LABEL'\n",
    "\n",
    "# Az adatbázisban található értékek megfeleltetése a programban\n",
    "LABELS = {\n",
    "    \"NEG\": 0,\n",
    "    \"SEM\": 1,\n",
    "    \"POZ\": 2\n",
    "}\n",
    "\n",
    "# Maximális tokensorozat méret\n",
    "MAX_SEQUENCE_LENGTH = 64\n",
    "\n",
    "# Tanítás során alkalmazott batch méret\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Tanítás során alkalmazott epoch szám. Korai megállás esetén nem feltétlenül jut el a megadott értékig a program\n",
    "EPOCHS = 30\n",
    "\n",
    "# Maximum hány TRAIN dokumentum kerüljön tokenizálásra. None esetén mind\n",
    "TRAIN_PROCESSED_MAX_DOCUMENTS = None\n",
    "\n",
    "# Maximum hány TEST dokumentum kerüljön tokenizálásra. None esetén mind\n",
    "TEST_PROCESSED_MAX_DOCUMENTS = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random seed-ek állítása\n",
    "\n",
    "if not USE_GPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "os.environ['PYTHONHASHSEED']=str(PYTHON_HASH_SEED)\n",
    "np.random.seed(NP_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(TF_SEED)\n",
    "tf.compat.v1.set_random_seed(TF_SEED)\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_DISABLE_SEGMENT_REDUCTION_OP_DETERMINISM_EXCEPTIONS'] = '1'\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GPU beállítása\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "if physical_devices:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Mappastruktúra beállítása\n",
    "\n",
    "path = 'checkpoints/' + CHECKPOINT_SUBDIR + CHECKPOINT_PREFIX\n",
    "\n",
    "ITER = \"1\"\n",
    "if not os.path.exists(path):\n",
    "  os.makedirs(path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Szöveg feldolgozására alkalmas metódusok\n",
    "\n",
    "def extend_context(text, context, left_index, right_index, context_length):\n",
    "    \"\"\"\n",
    "    Kontextus összeállításáért felelős metódus.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str | list(str)\n",
    "        A teljes szöveg, amelyből a kontextust kinyerjük.\n",
    "    context: str\n",
    "        Kezdetben csak célszemélyt tartalmazza, később a teljes kontextust.\n",
    "    left_index: int\n",
    "        Kezdetben a célszemély kezdőindexe a szövegben.\n",
    "    right_index: int\n",
    "        Kezdetben a célszemély utolsó tokenjének indexe a szövegben.\n",
    "    context_length: int\n",
    "        Mekkora legyen a kontextusméret.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Az összeállított kontextus.\n",
    "    \"\"\"\n",
    "\n",
    "    if context_length is None:\n",
    "        context_length = len(text)\n",
    "    if type(text) != list:\n",
    "        text = text.split()\n",
    "    start_size = 0\n",
    "    while start_size < context_length:\n",
    "        if left_index > 0:\n",
    "            left_index -= 1\n",
    "            context.insert(0, text[left_index])\n",
    "            if len(text[left_index]) > 1:\n",
    "                start_size += 1\n",
    "        if right_index < len(text) -1:\n",
    "            right_index += 1\n",
    "            context.append(text[right_index])\n",
    "            if len(text[right_index]) > 1:\n",
    "                start_size += 1\n",
    "        if left_index <= 0 and right_index >= len(text) -1:\n",
    "            break\n",
    "    return ' '.join(context)\n",
    "\n",
    "\n",
    "def contextualize(index, context_length=None):\n",
    "    \"\"\"\n",
    "    Metódus, amely inicializálja a kontextus összeállítását.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    index: int\n",
    "        Annak a dokumentumnak az indexe, amelynek a kontextusát szeretnénk kinyerni.\n",
    "    context_length: int, optional\n",
    "        Kontextus mérete. None esetén a teljes dokumentum a kontextus (default is None).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Az összeállított kontextus.\n",
    "    \"\"\"\n",
    "\n",
    "    text = ' '.join([i for i in re.split(r'( - |(?![%.-])\\W)|(-e[\\n ])', dataset[TEXT].iloc[index]) if i])\n",
    "    context_start_index = int(dataset[START_TOKEN].iloc[index] - 1)\n",
    "    context_stop_index = int(context_start_index+dataset[TOKEN_LEN].iloc[index] - 1)\n",
    "\n",
    "    context_name_tokens = \\\n",
    "        [t for t in text.split()[context_start_index:context_stop_index+1]]\n",
    "\n",
    "    context_list = context_name_tokens\n",
    "    left_index = context_start_index\n",
    "    right_index = context_stop_index\n",
    "    context = extend_context(text, context_list, left_index, right_index, context_length)\n",
    "    return context\n",
    "\n",
    "def delete_labeled_rows(dataset, label, percentage_to_remain = 10):\n",
    "    \"\"\"\n",
    "    Metódus, amely a paraméterben kapott címkével ellátott adatoknak csak a megadott százalékát tartja meg\n",
    "    minden más adat törlésre kerül. A törlésre kerülő sorok véletlenszerűen kerülnek kiválasztásra.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: pandas.DataFrame\n",
    "        Az adatbázisból készített DataFrame.\n",
    "    label: int\n",
    "        A vizsgált címke, amire a törlés végre lesz hajtva.\n",
    "    percentage_to_remain: int, optional\n",
    "        Megadható, hogy a DataFrame-ben a megadott címkének hány százaléka maradjon a DataFrameben (default is 10).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Az a DataFrame, amely az eldobott sorokat már nem tartalmazza az adott címkéből.\n",
    "    \"\"\"\n",
    "\n",
    "    neutral_ids = dataset.index[dataset[Y_HEADER] == label].tolist()\n",
    "    neutral_id_drop = set(random.sample(range(len(neutral_ids)), int(len(neutral_ids) * percentage_to_remain / 100)))\n",
    "    ids_to_delete = [x for i,x in enumerate(neutral_ids) if not i in neutral_id_drop]\n",
    "    return dataset.drop(ids_to_delete)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adatok előkészítése\n",
    "\n",
    "dataset = pd.read_csv('db/train.csv', sep=';')\n",
    "dataset[Y_HEADER] = dataset[Y_HEADER].map(LABELS)\n",
    "dataset = shuffle(dataset, random_state=SHUFFLE_RANDOM_STATE)\n",
    "dataset.info()\n",
    "\n",
    "dataset = delete_labeled_rows(dataset, LABELS['SEM'], percentage_to_remain=SEMLEGES_PERCENTAGE_TO_KEEP)\n",
    "dataset = delete_labeled_rows(dataset, LABELS['NEG'], percentage_to_remain=NEGATIV_PERCENTAGE_TO_KEEP)\n",
    "dataset = delete_labeled_rows(dataset, LABELS['POZ'], percentage_to_remain=POZITIV_PERCENTAGE_TO_KEEP)\n",
    "\n",
    "X_list = []\n",
    "print(len(dataset.index))\n",
    "for i  in range(len(dataset.index)):\n",
    "    X_list.append(contextualize(i, context_length=CONTEXT_LENGTH))\n",
    "\n",
    "print(X_list[:10])\n",
    "X = np.asarray(X_list)\n",
    "y = dataset[Y_HEADER].values\n",
    "\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.8, random_state=TRAIN_RANDOM_STATE)\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_rem, y_rem, train_size=0.5, random_state=TEST_RANDOM_STATE)\n",
    "\n",
    "\n",
    "y_train_labels = y_train\n",
    "y_dev_labels = y_dev\n",
    "y_test_labels = y_test\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_dev = to_categorical(y_dev, 3)\n",
    "y_test = to_categorical(y_test, 3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adatok vizualizálása\n",
    "\n",
    "def plot_label_counts(y, title='y labels'):\n",
    "    \"\"\"\n",
    "    Adatok eloszlásának ábrázolása.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        Az adathalmaz címkéi, amelyre a vizualizálás történik.\n",
    "    title: str\n",
    "        Az elkészített ábra címe.\n",
    "    \"\"\"\n",
    "\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    b = dict(zip(unique, counts))\n",
    "    plt.barh(range(len(b)), list(b.values()), align='center', color=['pink', 'lightblue', 'lightgreen'])\n",
    "    y_values = [\"Negatív\", \"Semleges\", \"Pozitív\"]\n",
    "    y_axis = np.arange(0, 3, 1)\n",
    "    plt.yticks(y_axis, y_values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Number of Samples in training Set')\n",
    "    plt.ylabel('Label')\n",
    "    ax = plt.gca()\n",
    "    for i, v in enumerate(b.values()):\n",
    "        plt.text(ax.get_xlim()[1]/100, i, str(v), color='blue', fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "plot_label_counts(y_train_labels, 'Train eloszlas')\n",
    "plot_label_counts(y_dev_labels, 'Dev eloszlas')\n",
    "plot_label_counts(y_test_labels, 'Test eloszlas')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# huBERT betöltése a Hugging Faces Model Hub-ból\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\", num_labels=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stop word-ök letöltése\n",
    "\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stop word-ök alkalmazásának előkészítése\n",
    "\n",
    "all_stopwords = []\n",
    "\n",
    "def apply_stopwords(sentences):\n",
    "    \"\"\"\n",
    "    Stop word-ök alkalmazása magyar nyelvű korpuszra.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences: list of numpy.ndarray\n",
    "        Az adathalmazban található dokumentumokat tartalmazó lista, amelyen a stop word-ök alkalmazva lesznek.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        A stop word-öket már nem tartalmazó dokumentumokat tartalmazó lista.\n",
    "    \"\"\"\n",
    "\n",
    "    global all_stopwords\n",
    "    corpus = []\n",
    "    for sen in sentences:\n",
    "        sentence = sen.split()\n",
    "        all_stopwords = stopwords.words('hungarian')\n",
    "        whitelist = [\"ne\", \"nem\", \"se\", \"sem\"]\n",
    "        sentence = [word for word in sentence if (word.lower() not in all_stopwords or word.lower() in whitelist)\n",
    "                 and len(word) > 1]\n",
    "        sentence = ' '.join(sentence)\n",
    "        corpus.append(sentence)\n",
    "    return corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dokumentumok tokenizálása\n",
    "\n",
    "def batch_encode(X):\n",
    "    \"\"\"\n",
    "    Tokenizálás a huBERT tokenizálóval.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: list of str\n",
    "        Az adathalmazban található dokumentumok listája, amelyek tokenizálásra kerülnek.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    transformers.tokenization_utils_base.BatchEncoding\n",
    "        A tokenizált dokumentumok.\n",
    "    \"\"\"\n",
    "\n",
    "    return bert_tokenizer.batch_encode_plus(\n",
    "    X,\n",
    "    truncation=True,\n",
    "    max_length=MAX_SEQUENCE_LENGTH,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length',\n",
    "    return_tensors='tf'\n",
    ")\n",
    "X_train = X_train.tolist() if not USE_STOPWORDS else apply_stopwords(X_train.tolist())\n",
    "X_dev = X_dev.tolist() if not USE_STOPWORDS else apply_stopwords(X_dev.tolist())\n",
    "X_test = X_test.tolist() if not USE_STOPWORDS else apply_stopwords(X_test.tolist())\n",
    "\n",
    "\n",
    "X_train = batch_encode(X_train[:TRAIN_PROCESSED_MAX_DOCUMENTS])\n",
    "X_dev = batch_encode(X_dev[:TEST_PROCESSED_MAX_DOCUMENTS])\n",
    "X_test = batch_encode(X_test[:TEST_PROCESSED_MAX_DOCUMENTS])\n",
    "y_train = y_train[:TRAIN_PROCESSED_MAX_DOCUMENTS]\n",
    "y_dev = y_dev[:TEST_PROCESSED_MAX_DOCUMENTS]\n",
    "y_test = y_test[:TEST_PROCESSED_MAX_DOCUMENTS]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Modell létrehozása\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Modell létrehozása, melynek inputja az input id-kat, illetve az attention mask-ot tartalmazó\n",
    "    réteg, tartalmazza a BERT modellt, kimeneti rétege osztályozára alkalmas.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    keras.engine.functional.Functional\n",
    "        Az elkészített modell.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(64,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input((64,), dtype=tf.int32, name='attention_mask')\n",
    "    output = bert_model([input_ids, attention_mask])[0]\n",
    "    output = tf.keras.layers.Dropout(rate=0.15)(output)\n",
    "    output = tf.keras.layers.Dense(3, activation='softmax')(output)\n",
    "    result = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    return result\n",
    "\n",
    "model = create_model()\n",
    "print(type(model))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Modell adatainak kiiratása\n",
    "\n",
    "print(bert_model.config)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checkpoint callback beállítása a modell checkpontjainak lementéséhez\n",
    "\n",
    "checkpoint_path = path + 'cp.ckpt'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tanítás során alkalmazott metódusok\n",
    "\n",
    "def calculate_weights():\n",
    "    \"\"\"\n",
    "    Megadja a címkék eloszlásának egymáshoz viszonyított súlyát.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict of numpy.float64\n",
    "        A címkék súlyait tartalmazó dict.\n",
    "    \"\"\"\n",
    "\n",
    "    y_categorical = np.argmax(y_train, axis=-1)\n",
    "    class_weights = list(class_weight.compute_class_weight('balanced', classes=np.unique(dataset[Y_HEADER]), y=y_categorical))\n",
    "    weights={}\n",
    "\n",
    "    for index, weight in enumerate(class_weights) :\n",
    "        weights[index]=weight\n",
    "\n",
    "    return weights\n",
    "\n",
    "def get_history_as_text(history, epoch):\n",
    "    \"\"\"\n",
    "    Visszaadja a kiírandó és fájlba mentendő stringet, amely a tanítás history-ját tartalmazza.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A kiírandó és fájlba mentendő stringet, amely a tanítás history-ját tartalmazza.\n",
    "    \"\"\"\n",
    "\n",
    "    return f'Epoch {epoch+1: <3}: loss: {format(history[\"loss\"][epoch], \".4f\")} - accuracy: {format(history[\"accuracy\"][epoch],\".4f\")} - val_loss: {format(history[\"val_loss\"][epoch], \".4f\")} - val_accuracy: {format(history[\"val_accuracy\"][epoch], \".4f\")}'\n",
    "\n",
    "def fit_model():\n",
    "    \"\"\"\n",
    "    Modell illesztését elvégző metódus. Amennyiben a LOAD_CHECKPOINT True, úgy illesztés helyett a mentett tanítást\n",
    "    tölti be és annak eredményével tér vissza.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    history: dict of fit results\n",
    "        Az illesztés során az epoch-ok információit tartalmazó dic, mely tartalmazza a\n",
    "        loss, az accuracy, a val_loss és a val_accuracy adatokat.\n",
    "    result: list of float\n",
    "        A modell kiértékelését tartalmazza a teszt adathalmazra.\n",
    "    predict: numpy.ndarray\n",
    "        A modell predikcióit tartalmazza a teszt adathalmazra.\n",
    "    \"\"\"\n",
    "\n",
    "    if USE_CLASS_WEIGHTS:\n",
    "        weights = calculate_weights()\n",
    "        print('weights: ', weights)\n",
    "    else:\n",
    "        weights = {0:1, 1:1, 2:1}\n",
    "    if not LOAD_CHECKPOINT:\n",
    "        early_stopping_callback = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1,\n",
    "            patience=2,\n",
    "            restore_best_weights=RESTORE_BEST_WEIGHTS)\n",
    "        if ITER == \"9\":\n",
    "            history = model.fit(\n",
    "                x=X_train.values(),\n",
    "                class_weight=weights,\n",
    "                y=y_train,\n",
    "                validation_data=(X_dev.values(), y_dev),\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                callbacks=[early_stopping_callback, cp_callback]\n",
    "            )\n",
    "        else:\n",
    "            history = model.fit(\n",
    "                x=X_train.values(),\n",
    "                class_weight=weights,\n",
    "                y=y_train,\n",
    "                validation_data=(X_dev.values(), y_dev),\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                callbacks=[early_stopping_callback]\n",
    "            )\n",
    "        with open(path + '.history', 'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)\n",
    "        text_file = open(path + \"history\" + ITER + \".txt\", \"w\")\n",
    "        print(len(history.history['loss']))\n",
    "        for i in range(len(history.history['loss'])):\n",
    "            history_text = get_history_as_text(history.history, i)\n",
    "            print(history_text)\n",
    "            text_file.writelines(history_text + '\\n')\n",
    "        text_file.close()\n",
    "    else:\n",
    "        model.load_weights(checkpoint_path)\n",
    "        history = pickle.load(open(path + '.history', \"rb\"))\n",
    "        for i in range(len(history['loss'])):\n",
    "            history_text = get_history_as_text(history, i)\n",
    "            print(history_text)\n",
    "    result = model.evaluate(X_test.values(), y_test)\n",
    "    predict = model.predict(X_test.values())\n",
    "    np_predict = np.argmax(predict,axis=1)\n",
    "    return history, result, np_predict\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Kiértékelés vizualizálása szövegesen és ábrán\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "def evaluate(predict, result, y):\n",
    "    \"\"\"\n",
    "    A modell kiértékelése. Kiíratja az osztályozási eredményeket, a pontosságot, illetve az igazságmátrixot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predict: numpy.ndarray\n",
    "        A modell predikcióit tartalmazza a teszt adathalmazra.\n",
    "    result: list of float\n",
    "        A modell kiértékelését tartalmazza a teszt adathalmazra.\n",
    "    y: numpy.ndarray\n",
    "        A teszt adathalmaz címkéit tartalmazza.\n",
    "    \"\"\"\n",
    "\n",
    "    y_le = le.fit_transform(y[:TEST_PROCESSED_MAX_DOCUMENTS])\n",
    "    print('Classification report:')\n",
    "    print(classification_report(y_le, predict))\n",
    "    print(f'Accuracy: {accuracy_score(y_le, predict): >43}')\n",
    "    print(f'Accuracy from evaluation: {result[1]: >27}')\n",
    "    print('Confusion matrix:')\n",
    "    df_cm = pd.DataFrame(confusion_matrix(y_le, predict),\n",
    "                         index=[i for i in ['negative', 'neutral', 'positive']],\n",
    "                         columns=[i for i in ['negative', 'neutral', 'positive']])\n",
    "    if not LOAD_CHECKPOINT:\n",
    "        with open(path + \"results\" + ITER + \".txt\", \"w\") as text_file:\n",
    "            df_cm_string = df_cm.to_string(header=False, index=False)\n",
    "            text_file.write('Classification report:\\n')\n",
    "            text_file.write(classification_report(y_le, predict))\n",
    "            text_file.write(f'\\nAccuracy: {accuracy_score(y_le, predict): >43}\\n')\n",
    "            text_file.write(f'Accuracy from evaluation: {result[1]: >27}\\n')\n",
    "            text_file.write('\\nConfusion matrix:\\n')\n",
    "            text_file.write(df_cm_string)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.title(CHECKPOINT_PREFIX[:-1])\n",
    "    hm = sn.heatmap(df_cm, annot=True, fmt='g', cmap=\"Blues\")\n",
    "    hm.set(ylabel='True label', xlabel='Predicted label')\n",
    "    if not LOAD_CHECKPOINT:\n",
    "        plt.savefig(path + 'accuracy-' + format(result[1], \".4f\") + ITER + '.jpg')\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Modell illesztése és kiértékelése 9 iterációban.\n",
    "\n",
    "if not LOAD_CHECKPOINT:\n",
    "    for i in range(1,10):\n",
    "        ITER = str(i)\n",
    "        print(\"Iteration \", ITER)\n",
    "        history, result, predict = fit_model()\n",
    "        evaluate(predict, result, y_test_labels)\n",
    "else:\n",
    "    history, result, predict = fit_model()\n",
    "    evaluate(predict, result, y_test_labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}