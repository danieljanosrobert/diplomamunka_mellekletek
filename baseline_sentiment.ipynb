{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gyakran változtatott konstansok\n",
    "\n",
    "# Mekkora kontextusra történjen az illesztés\n",
    "CONTEXT_LENGTH = None\n",
    "\n",
    "# A teljes adatbázis pozitív címkéjű adatainak hány százalékával történjen a futás\n",
    "POZITIV_PERCENTAGE_TO_KEEP = 100\n",
    "\n",
    "# A teljes adatbázis negatív címkéjű adatainak hány százalékával történjen a futás\n",
    "NEGATIV_PERCENTAGE_TO_KEEP = 50\n",
    "\n",
    "# A teljes adatbázis semleges címkéjű adatainak hány százalékával történjen a futás\n",
    "SEMLEGES_PERCENTAGE_TO_KEEP = 12\n",
    "\n",
    "# Stop word-ök használata\n",
    "USE_STOPWORDS = True\n",
    "\n",
    "# Stemmelés használata\n",
    "USE_STEM = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importok\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "from nltk import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Konstansok\n",
    "\n",
    "# Random seed értéke\n",
    "SEED = 42\n",
    "\n",
    "# Könyvtárspecifikus random seed-ek értéke\n",
    "SHUFFLE_RANDOM_STATE = SEED\n",
    "TRAIN_RANDOM_STATE = SEED\n",
    "RANDOM_SEED = SEED\n",
    "NP_SEED = SEED\n",
    "TF_SEED = SEED\n",
    "PYTHON_HASH_SEED = SEED\n",
    "SGD_RANDOM_STATE = SEED\n",
    "\n",
    "# A program számára fontos fejlécek értéke az adatbázisban\n",
    "TEXT = 'Sentence'\n",
    "FILTER = 'Entity'\n",
    "START_TOKEN = 'START'\n",
    "TOKEN_LEN = 'LEN'\n",
    "Y_HEADER = 'LABEL'\n",
    "\n",
    "# Az adatbázisban található értékek megfeleltetése a programban\n",
    "LABELS = {\n",
    "    \"NEG\": 0,\n",
    "    \"SEM\": 1,\n",
    "    \"POZ\": 2\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random seed-ek állítása\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(PYTHON_HASH_SEED)\n",
    "np.random.seed(NP_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(TF_SEED)\n",
    "tf.compat.v1.set_random_seed(TF_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Text processing\n",
    "\n",
    "def extend_context(text, context, left_index, right_index, context_length):\n",
    "    \"\"\"\n",
    "    Kontextus összeállításáért felelős metódus.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str | list(str)\n",
    "        A teljes szöveg, amelyből a kontextust kinyerjük.\n",
    "    context: str\n",
    "        Kezdetben csak célszemélyt tartalmazza, később a teljes kontextust.\n",
    "    left_index: int\n",
    "        Kezdetben a célszemély kezdőindexe a szövegben.\n",
    "    right_index: int\n",
    "        Kezdetben a célszemély utolsó tokenjének indexe a szövegben.\n",
    "    context_length: int\n",
    "        Mekkora legyen a kontextusméret.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Az összeállított kontextus.\n",
    "    \"\"\"\n",
    "\n",
    "    if context_length is None:\n",
    "        context_length = len(text)\n",
    "    if type(text) != list:\n",
    "        text = text.split()\n",
    "    start_size = 0\n",
    "    while start_size < context_length:\n",
    "        if left_index > 0:\n",
    "            left_index -= 1\n",
    "            context.insert(0, text[left_index])\n",
    "            if len(text[left_index]) > 1:\n",
    "                start_size += 1\n",
    "        if right_index < len(text) -1:\n",
    "            right_index += 1\n",
    "            context.append(text[right_index])\n",
    "            if len(text[right_index]) > 1:\n",
    "                start_size += 1\n",
    "        if left_index <= 0 and right_index >= len(text) -1:\n",
    "            break\n",
    "    return ' '.join(context)\n",
    "\n",
    "\n",
    "def contextualize(index, context_length=None):\n",
    "    \"\"\"\n",
    "    Metódus, amely inicializálja a kontextus összeállítását.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    index: int\n",
    "        Annak a dokumentumnak az indexe, amelynek a kontextusát szeretnénk kinyerni.\n",
    "    context_length: int, optional\n",
    "        Kontextus mérete. None esetén a teljes dokumentum a kontextus (default is None).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Az összeállított kontextus.\n",
    "    \"\"\"\n",
    "\n",
    "    text = ' '.join([i for i in re.split(r'( - |(?![%.-])\\W)|(-e[\\n ])', dataset[TEXT].iloc[index]) if i])\n",
    "    context_start_index = int(dataset[START_TOKEN].iloc[index] - 1)\n",
    "    context_stop_index = int(context_start_index+dataset[TOKEN_LEN].iloc[index] - 1)\n",
    "\n",
    "    context_name_tokens = \\\n",
    "        [t for t in text.split()[context_start_index:context_stop_index+1]]\n",
    "\n",
    "    context_list = context_name_tokens\n",
    "    left_index = context_start_index\n",
    "    right_index = context_stop_index\n",
    "    context = extend_context(text, context_list, left_index, right_index, context_length)\n",
    "    return context\n",
    "\n",
    "def delete_labeled_rows(dataset, label, percentage_to_remain = 10):\n",
    "    \"\"\"\n",
    "    Metódus, amely a paraméterben kapott címkével ellátott adatoknak csak a megadott százalékát tartja meg\n",
    "    minden más adat törlésre kerül. A törlésre kerülő sorok véletlenszerűen kerülnek kiválasztásra.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: pandas.DataFrame\n",
    "        Az adatbázisból készített DataFrame.\n",
    "    label: int\n",
    "        A vizsgált címke, amire a törlés végre lesz hajtva.\n",
    "    percentage_to_remain: int, optional\n",
    "        Megadható, hogy a DataFrame-ben a megadott címkének hány százaléka maradjon a DataFrameben (default is 10).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Az a DataFrame, amely az eldobott sorokat már nem tartalmazza az adott címkéből.\n",
    "    \"\"\"\n",
    "\n",
    "    label_mapped = dataset[Y_HEADER].map(LABELS)\n",
    "    neutral_ids = dataset.index[label_mapped == label].tolist()\n",
    "    neutral_id_drop = set(random.sample(range(len(neutral_ids)), int(len(neutral_ids) * percentage_to_remain / 100)))\n",
    "    ids_to_delete = [x for i,x in enumerate(neutral_ids) if not i in neutral_id_drop]\n",
    "    return dataset.drop(ids_to_delete)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adatok előkészítése\n",
    "\n",
    "dataset = pd.read_csv('db/train.csv', sep=';')\n",
    "dataset = shuffle(dataset, random_state=SHUFFLE_RANDOM_STATE)\n",
    "dataset.info()\n",
    "\n",
    "dataset = delete_labeled_rows(dataset, LABELS['SEM'], percentage_to_remain=SEMLEGES_PERCENTAGE_TO_KEEP)\n",
    "dataset = delete_labeled_rows(dataset, LABELS['NEG'], percentage_to_remain=NEGATIV_PERCENTAGE_TO_KEEP)\n",
    "dataset = delete_labeled_rows(dataset, LABELS['POZ'], percentage_to_remain=POZITIV_PERCENTAGE_TO_KEEP)\n",
    "\n",
    "X_list = []\n",
    "print(len(dataset.index))\n",
    "for i  in range(len(dataset.index)):\n",
    "    X_list.append(contextualize(i))\n",
    "\n",
    "X = np.asarray(X_list)\n",
    "y = dataset[Y_HEADER].values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=TRAIN_RANDOM_STATE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adatok vizualizálása\n",
    "\n",
    "def plot_label_counts(y, title='y labels'):\n",
    "    \"\"\"\n",
    "    Adatok eloszlásának ábrázolása.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        Az adathalmaz címkéi, amelyre a vizualizálás történik.\n",
    "    title: str\n",
    "        Az elkészített ábra címe.\n",
    "    \"\"\"\n",
    "\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    b = dict(zip(unique, counts))\n",
    "    b_values = [b['NEG'], b['SEM'], b['POZ']]\n",
    "    print(b.values())\n",
    "    plt.barh(range(len(b)), b_values, align='center', color=['pink', 'lightblue', 'lightgreen'])\n",
    "    y_values = [\"Negatív\", \"Semleges\", \"Pozitív\"]\n",
    "    y_axis = np.arange(0, 3, 1)\n",
    "    plt.yticks(y_axis, y_values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Number of Samples in training Set')\n",
    "    plt.ylabel('Label')\n",
    "    ax = plt.gca()\n",
    "    for i, v in enumerate(b_values):\n",
    "        plt.text(ax.get_xlim()[1]/100, i, str(v), color='blue', fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "plot_label_counts(y_train, 'Train eloszlas')\n",
    "plot_label_counts(y_test, 'Test eloszlas')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Kiértékelés vizualizálása szövegesen és ábrán\n",
    "\n",
    "def evaluate(predict, labels):\n",
    "    \"\"\"\n",
    "    A modell kiértékelése. Kiíratja az osztályozási eredményeket, a pontosságot, illetve az igazságmátrixot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predict: numpy.ndarray\n",
    "        A modell predikcióit tartalmazza a teszt adathalmazra.\n",
    "    y: numpy.ndarray\n",
    "        A teszt adathalmaz címkéit tartalmazza.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Classification report:')\n",
    "    print(classification_report(labels, predict))\n",
    "    print('Accuracy:')\n",
    "    print(accuracy_score(labels, predict))\n",
    "\n",
    "    print('Confusion matrix:')\n",
    "    df_cm = pd.DataFrame(confusion_matrix(labels, predict, labels=['POZ', 'SEM', 'NEG']),\n",
    "                         index=[i for i in ['POZ', 'SEM', 'NEG']],\n",
    "                         columns=[i for i in ['POZ', 'SEM', 'NEG']])\n",
    "    plt.figure(figsize=(10,7))\n",
    "    hm = sn.heatmap(df_cm, annot=True, fmt='g', cmap=\"Blues\")\n",
    "    hm.set(ylabel='True label', xlabel='Predicted label')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stop word-ök letöltése\n",
    "\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Korpuszok létrehozása a dokumentumokból\n",
    "\n",
    "all_stopwords = []\n",
    "\n",
    "def create_corpus_by_dataset(sentences):\n",
    "    \"\"\"\n",
    "    Korpusz létrehozása. Stop word-ök és stemmelés alkalmazása, amennyiben azok engedélyezve vannak.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences: list of numpy.ndarray\n",
    "        Az adathalmazban található dokumentumokat tartalmazó lista, amelyen a stop word-ök alkalmazva lesznek.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        Az elkészített korpusz.\n",
    "    \"\"\"\n",
    "\n",
    "    global all_stopwords\n",
    "    corpus = []\n",
    "    for sen in sentences:\n",
    "        sentence = re.sub('[^a-zA-ZöÖüÜóÓőŐúÚáÁűŰéÉíÍ]', ' ', sen)\n",
    "        sentence = sentence.lower().split()\n",
    "        if USE_STOPWORDS:\n",
    "            all_stopwords = stopwords.words('hungarian')\n",
    "            whitelist = [\"ne\", \"nem\", \"se\", \"sem\"]\n",
    "            sentence = [word for word in sen.split() if (word not in all_stopwords or word in whitelist)\n",
    "                 and len(word) > 1]\n",
    "        if USE_STEM:\n",
    "            ps = SnowballStemmer('hungarian')\n",
    "            sentence = [ps.stem(word) for word in sentence]\n",
    "        sentence = ' '.join(sentence)\n",
    "        corpus.append(sentence)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "X_train_clean = create_corpus_by_dataset(X_train)\n",
    "X_test_clean = create_corpus_by_dataset(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bag of words modell létrehozása és illesztése\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=0.002, ngram_range=(1,3))\n",
    "x_train_model = cv.fit_transform(X_train_clean)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SGD osztályozó létrehozása és illesztése\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "classifier = SGDClassifier(random_state=SGD_RANDOM_STATE, learning_rate='constant',\n",
    "                           eta0=0.01)\n",
    "classifier.fit(x_train_model, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Beállított modellek kiértékelése\n",
    "\n",
    "x_dev_model = cv.transform(X_test_clean)\n",
    "\n",
    "dev_predict = classifier.predict(x_dev_model)\n",
    "\n",
    "evaluate(dev_predict, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grid Search előkészítése és illesztése\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "           ('vect', cv),\n",
    "           ('clf', classifier)\n",
    "])\n",
    "\n",
    "parameters_to_tune = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__min_df': (0.001, 0.002, 0.005, 0.01),\n",
    "    'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (1,3), (1,4)),\n",
    "    'clf__max_iter': (500, 1000, 2000, 5000),\n",
    "    'clf__eta0': (0.005, 0.01, 0.02, 0.05)\n",
    "}\n",
    "clf=GridSearchCV(pipeline,parameters_to_tune, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train_clean, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optimális hiperparaméterekkel beállított modellek részleteinek kiírása\n",
    "\n",
    "print(clf.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predikálás az optimális hiperparaméterekkel beállított modelleken\n",
    "\n",
    "pred2 = clf.predict(X_test_clean)\n",
    "evaluate(pred2, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}